{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Processing for CHIR Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up steps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data loading\n",
    "\n",
    "CHIR_df = pd.read_csv(\"CHIR_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIR Processed Data successfully exported to: derived_data/CHIR_processed_for_Airtable.csv\n"
     ]
    }
   ],
   "source": [
    "# setting up a data check for potential duplicates\n",
    "\n",
    "# Typology for duplicates (pending confirmation)\n",
    "dupe_code_list = {\"cross_state_duplicate\", \"document_type_duplicate\", \"non_duplicate\", \"payer_duplicate\",\"true_duplicate\"}\n",
    "\n",
    "# Converting the policy identifier column to a more useful format (numeric to string)\n",
    "CHIR_df['Policy Identifier'] = CHIR_df['Policy Identifier'].astype(str)\n",
    "\n",
    "# Creating a duplicate flagger function for this data stream\n",
    "# This flags duplicates in the DataFrame based on Policy Identifier, States, Document Type, and Payer.\n",
    "\n",
    "def flag_duplicates(CHIR_df):\n",
    "\n",
    "    # Sort the DataFrame to ensure consistent grouping for duplicates\n",
    "    CHIR_df_sorted = CHIR_df.sort_values(by=[\"Policy Identifier\", \"States\", \"Document Type\", \"Payer\"])\n",
    "\n",
    "    # Initialize the 'duplicate_flagger' column with a default value\n",
    "    CHIR_df_sorted[\"duplicate_flagger\"] = \"non_duplicate\"\n",
    "\n",
    "    # Identify where there are the same 'Policy Identifier' but different 'States'.\n",
    "    cross_state_dupes = CHIR_df_sorted.groupby(\"Policy Identifier\").filter(\n",
    "        lambda x: x[\"States\"].nunique() > 1\n",
    "    )\n",
    "    CHIR_df_sorted.loc[cross_state_dupes.index, \"duplicate_flagger\"] = \"cross_state_duplicate\"\n",
    "    \n",
    "\n",
    "    # Identify where there are the same 'Policy Identifier' and 'States' but different 'Document Type'.\n",
    "    doc_type_dupes_mask = (\n",
    "        (CHIR_df_sorted.duplicated(subset=[\"Policy Identifier\", \"States\"], keep=False))\n",
    "        & (\n",
    "            ~CHIR_df_sorted.duplicated(\n",
    "                subset=[\"Policy Identifier\", \"States\", \"Document Type\"], keep=False\n",
    "            )\n",
    "        )\n",
    "        & (CHIR_df_sorted[\"duplicate_flagger\"] == \"non_duplicate\")\n",
    "    )\n",
    "    CHIR_df_sorted.loc[doc_type_dupes_mask, \"duplicate_flagger\"] = \"document_type_duplicate\"\n",
    "    \n",
    "\n",
    "\n",
    "    # Identify where there are the same 'Policy Identifier' and 'States' but different 'Payer'.\n",
    "    payer_dupes_mask = (\n",
    "        (CHIR_df_sorted.duplicated(subset=[\"Policy Identifier\", \"States\"], keep=False))\n",
    "        & (\n",
    "            ~CHIR_df_sorted.duplicated(\n",
    "                subset=[\"Policy Identifier\", \"States\", \"Payer\"], keep=False\n",
    "            )\n",
    "        )\n",
    "        & (CHIR_df_sorted[\"duplicate_flagger\"] == \"non_duplicate\")\n",
    "    )\n",
    "    CHIR_df_sorted.loc[payer_dupes_mask, \"duplicate_flagger\"] = \"payer_duplicate\"\n",
    "    \n",
    "\n",
    "    # Identifying true duplicates, where all these factors are identical\n",
    "    true_dupes_mask = (\n",
    "        CHIR_df_sorted.duplicated(\n",
    "            subset=[\"Policy Identifier\", \"States\", \"Document Type\", \"Payer\"], keep=False\n",
    "        )\n",
    "        & (CHIR_df_sorted[\"duplicate_flagger\"] == \"non_duplicate\")\n",
    "    )\n",
    "    CHIR_df_sorted.loc[true_dupes_mask, \"duplicate_flagger\"] = \"true_duplicate\"\n",
    "    \n",
    "\n",
    "    return CHIR_df_sorted\n",
    "\n",
    "# Running the function on the CHIR data\n",
    "dupecheck_CHIR_df = flag_duplicates(CHIR_df.copy())\n",
    "\n",
    "# Exporting the modified version\n",
    "output_path = \"derived_data/CHIR_processed_for_Airtable.csv\"\n",
    "dupecheck_CHIR_df.to_csv(output_path, index= False)\n",
    "\n",
    "print(f\"CHIR Processed Data successfully exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIR Categorized CGM Data successfully exported to: derived_data/cat_CHIR_processed_for_Airtable.csv\n"
     ]
    }
   ],
   "source": [
    "# CGM grouping exercise\n",
    "# This categorizes CGM product coverage based on if they are 'Dexcom' and 'FreeStyle'\n",
    "\n",
    "\n",
    "def categorize_cgm_coverage(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "\n",
    "    # Create an empty list to store the labels for the new column\n",
    "    cgm_grouping_labels = []\n",
    "\n",
    "    # Iterate through each cell in the specified column\n",
    "    for cell_value in df[column_name]:\n",
    "        # Initialize flags for Dexcom and FreeStyle presence\n",
    "        has_dexcom = False\n",
    "        has_freestyle = False\n",
    "\n",
    "        # Handle missing or empty values first\n",
    "        if pd.isna(cell_value) or str(cell_value).strip() == '':\n",
    "            cgm_grouping_labels.append(\"No coverage\")\n",
    "            continue # Move to the next cell\n",
    "\n",
    "        # Convert the cell value to a string and then to lowercase for case-insensitive matching\n",
    "        # Split the string by commas to get individual product names\n",
    "        products = str(cell_value).lower().split(',')\n",
    "\n",
    "        # Check for the presence of 'dexcom' and 'freestyle' in any of the product names\n",
    "        for product in products:\n",
    "            if \"dexcom\" in product:\n",
    "                has_dexcom = True\n",
    "            if \"freestyle\" in product:\n",
    "                has_freestyle = True\n",
    "\n",
    "        # Apply the labeling logic based on the flags\n",
    "        if has_dexcom and has_freestyle:\n",
    "            cgm_grouping_labels.append(\"both Dexcom and FreeStyle\")\n",
    "        elif has_dexcom:\n",
    "            cgm_grouping_labels.append(\"Dexcom only\")\n",
    "        elif has_freestyle:\n",
    "            cgm_grouping_labels.append(\"FreeStyle only\")\n",
    "        else:\n",
    "            # If neither Dexcom nor FreeStyle are found, but the cell was not empty,\n",
    "            # it means other products are covered.\n",
    "            cgm_grouping_labels.append(\"Others covered\")\n",
    "\n",
    "    # Assign the generated labels to the new 'CGM_grouping' column in the DataFrame\n",
    "    df['CGM_grouping'] = cgm_grouping_labels\n",
    "\n",
    "    return df\n",
    "\n",
    "# test run\n",
    "cat_CHIR_df = categorize_cgm_coverage(dupecheck_CHIR_df.copy(), 'CGMs Covered')\n",
    "\n",
    "# Exporting the modified version\n",
    "output_path = \"derived_data/cat_CHIR_processed_for_Airtable.csv\"\n",
    "cat_CHIR_df.to_csv(output_path, index= False)\n",
    "\n",
    "print(f\"CHIR Categorized CGM Data successfully exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIR Implanted/non-implanted & counted CGM Data successfully exported to: derived_data/counted_and_implanted_CHIR_processed_for_Airtable.csv\n"
     ]
    }
   ],
   "source": [
    "# grouping for implanted\n",
    "\n",
    "# writing a dictionary based on the table\n",
    "device_implant_dict = {\n",
    "    \"Eversense E3 rtCGM\": \"implanted\",\n",
    "    \"Eversense 365\": \"implanted\",\n",
    "    \"Medtronic Guardian 3 rtCGM\": \"non-implanted\",\n",
    "    \"Medtronic Guardian 4 rtCGM\": \"non-implanted\",\n",
    "    \"Medtronic Simplera rtCGM\": \"non-implanted\",\n",
    "    \"Medtronic Simplera Sync rtCGM\": \"non-implanted\",\n",
    "    \"Dexcom G6 rtCGM\": \"non-implanted\",\n",
    "    \"Dexcom G7 rtCGM\": \"non-implanted\",\n",
    "    \"Abbott FreeStyle Libre 2 Plus rtCGM\": \"non-implanted\",\n",
    "    \"Abbott FreeStyle Libre 3 Plus rtCGM\": \"non-implanted\",\n",
    "    \"Abbott FreeStyle Libre 2 isCGM\": \"non-implanted\",\n",
    "    \"Abbott FreeStyle Libre 14 isCGM\": \"non-implanted\",\n",
    "    \"Unspecified - transcutaneous rtCGM\": \"non-implanted\",\n",
    "    \"Unspecified - transcutaneous isCGM\": \"non-implanted\",\n",
    "    \"Unspecified - implantable rtCGM\": \"implanted\",\n",
    "    \"Unspecified\": \"unknown\"\n",
    "}\n",
    "\n",
    "# iterating through our data (with a copy for safety)\n",
    "implanted_CHIR_df = cat_CHIR_df.dropna(subset=['CHIR Review Fields Last Modified']).copy()\n",
    "\n",
    "# building an iterating function to run through the data\n",
    "def categorize_implanted_type(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "\n",
    "    # create an empty list to store the labels for the new column\n",
    "    implanted_type_labels = []\n",
    "\n",
    "    # iterate through each cell in the specified column\n",
    "    for cell_value in df[column_name]:\n",
    "        # handling missing or empty values\n",
    "        if pd.isna(cell_value) or str(cell_value).strip() == '':\n",
    "            implanted_type_labels.append(\"no devices\")\n",
    "            continue\n",
    "\n",
    "        # separating multi-device coverage by commas\n",
    "        products_in_cell = str(cell_value).split(',')\n",
    "        \n",
    "        has_implanted_device = False\n",
    "        has_non_implanted_device = False\n",
    "        \n",
    "        # check in case any unknowns (or spelling variations)\n",
    "        found_any_known_device_in_cell = False \n",
    "\n",
    "        # check each product in the cell against the dictionary\n",
    "        for product_in_cell in products_in_cell:\n",
    "            for mapped_device_name, status in device_implant_dict.items():\n",
    "                # check for partial matches: if the mapped name is in the product string, or vice-versa\n",
    "                if mapped_device_name in product_in_cell or product_in_cell in mapped_device_name:\n",
    "                    found_any_known_device_in_cell = True\n",
    "                    if status == \"implanted\":\n",
    "                        has_implanted_device = True\n",
    "                    elif status == \"non-implanted\":\n",
    "                        has_non_implanted_device = True\n",
    "                    break\n",
    "\n",
    "        # applying the labeling logic based on the flags after checking all products in the cell\n",
    "        if has_implanted_device and has_non_implanted_device:\n",
    "            implanted_type_labels.append(\"both implanted and non-implanted\")\n",
    "        elif has_implanted_device:\n",
    "            implanted_type_labels.append(\"implanted only\")\n",
    "        elif has_non_implanted_device:\n",
    "            implanted_type_labels.append(\"non-implanted only\")\n",
    "        else:\n",
    "            implanted_type_labels.append(\"unknown device type\") \n",
    "\n",
    "    # assign the generated labels to the new 'Implanted_Type' column\n",
    "    df['Implanted_Type'] = implanted_type_labels\n",
    "\n",
    "    return df\n",
    "\n",
    "# test run with our copied data\n",
    "implanted_CHIR_df = categorize_implanted_type(implanted_CHIR_df, 'CGMs Covered')\n",
    "\n",
    "# supplementing the data to include a count of the number of devices\n",
    "def count_cell_items(df: pd.DataFrame, column_name: str, new_column_name: str) -> pd.DataFrame:\n",
    "    totals = []\n",
    "    for cell_value in df[column_name]:\n",
    "        if pd.isna(cell_value) or str(cell_value).strip() == '':\n",
    "            totals.append(0) # 0 for empty cells\n",
    "        else:\n",
    "            items = [item.strip() for item in str(cell_value).split(',') if item.strip()]\n",
    "            totals.append(len(items))\n",
    "    \n",
    "    df[new_column_name] = totals\n",
    "    return df    \n",
    "\n",
    "implanted_CHIR_df = count_cell_items(implanted_CHIR_df, 'CGMs Covered', 'CGM_count')\n",
    "\n",
    "# exporting the modified version\n",
    "new_output_path = \"derived_data/counted_and_implanted_CHIR_processed_for_Airtable.csv\"\n",
    "implanted_CHIR_df.to_csv(new_output_path, index= False)\n",
    "\n",
    "print(f\"CHIR Implanted/non-implanted & counted CGM Data successfully exported to: {new_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscraping and data organization -- pulling the CHIR documents\n",
    "# adding packages for scraping\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "def scrape_chir_pdfs(df, username, password):\n",
    "\n",
    "    # organizing the destination folder\n",
    "    download_folder = \"CHIR Policy Reporter Downloads\"\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "        print(f\"Created new folder: {os.path.abspath(download_folder)}\")\n",
    "\n",
    "    # need to use a special headless browser to overcome the two step login process\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\") \n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    try:\n",
    "        print(\"Navigating to login page with Selenium...\")\n",
    "        driver.get(\"https://portal.policyreporter.com/login\")\n",
    "\n",
    "        # Wait for username field and submit\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"username\"))\n",
    "        ).send_keys(username)\n",
    "        driver.find_element(By.XPATH, \"//input[@type='submit' and @value='Next']\").click()\n",
    "\n",
    "        # Wait for password field and submit\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"password\"))\n",
    "        ).send_keys(password)\n",
    "        driver.find_element(By.XPATH, \"//button[contains(text(), 'Submit')]\").click()\n",
    "\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        print(\"Login successful with Selenium.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Selenium login failed: {e}\")\n",
    "        driver.quit()\n",
    "        return\n",
    "        \n",
    "    # scraping function\n",
    "    for index, row in df.iterrows():\n",
    "            # based on the CHIR data frame set up earlier\n",
    "            dynamic_url = row['Link to Policy/Formulary PDF']\n",
    "            name = row['Document Title']\n",
    "            unique_id = row['airtable_uuid']\n",
    "            \n",
    "            # cleaing the filename to remove invalid characters and creating a destination file name based on the UUID\n",
    "            filename = f\"{name}_{unique_id}.pdf\".replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\").replace(\"?\", \"_\").replace(\"|\", \"_\")\n",
    "            filepath = os.path.join(download_folder, filename)\n",
    "\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\"File already exists, skipping: {filename}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # first, circumventing the dynamic URL gatekeeping permissions checker\n",
    "                print(f\"Retrieving PDF link from: {dynamic_url}\")\n",
    "                driver.get(dynamic_url)\n",
    "\n",
    "                # saving the new URL after the redirecting\n",
    "                final_pdf_url = driver.current_url\n",
    "\n",
    "                # Now, use requests with the cookies from the driver to download\n",
    "                s = requests.Session()\n",
    "                for cookie in driver.get_cookies():\n",
    "                    s.cookies.set(cookie['name'], cookie['value'])\n",
    "\n",
    "                # finally, we download that pdf!\n",
    "                response_pdf = s.get(final_pdf_url, stream=True)\n",
    "                response_pdf.raise_for_status()\n",
    "\n",
    "                # confirming that we are pulling the pdf and not the HTML file\n",
    "                content_type = response_pdf.headers.get('Content-Type', '')\n",
    "                if 'application/pdf' not in content_type:\n",
    "                    print(f\"❗ Warning: Expected a PDF but received a different content type ({content_type}) from {final_pdf_url}. Skipping file.\")\n",
    "                    continue\n",
    "\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    for chunk in response_pdf.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                print(f\"✅ Successfully downloaded: {filename}\")\n",
    "                \n",
    "                # adding a snooze to circumvent the usual anti-download blockers\n",
    "                sleep_time = random.uniform(5, 17)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❗ Failed to process URL {dynamic_url}: {e}\")\n",
    "    driver.quit()\n",
    "    print(\"All downloads complete.\")\n",
    "\n",
    "# running the code\n",
    "reboot_CHIR_df = CHIR_df.iloc[2450:]\n",
    "\n",
    "scrape_chir_pdfs(reboot_CHIR_df, \"aek35@georgetown.edu\", \"GeorgetownDiabetes1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 'Partial Hospitalization Program and Intensive Outpatient Program Services (CA Commercial)_recGiO5jn6apYiKo4.pdf' to record recGiO5jn6apYiKo4...\n",
      "✅ Successfully uploaded 'Partial Hospitalization Program and Intensive Outpatient Program Services (CA Commercial)_recGiO5jn6apYiKo4.pdf'.\n",
      "Uploading 'Ambulatory or Outpatient Surgery Center Procedures_rec6hWizOZMoCizxL.pdf' to record rec6hWizOZMoCizxL...\n",
      "✅ Successfully uploaded 'Ambulatory or Outpatient Surgery Center Procedures_rec6hWizOZMoCizxL.pdf'.\n",
      "Uploading 'Clinical Criteria (Colorado)_recwXGygv1b5pDPra.pdf' to record recwXGygv1b5pDPra...\n",
      "✅ Successfully uploaded 'Clinical Criteria (Colorado)_recwXGygv1b5pDPra.pdf'.\n",
      "Uploading 'Anthem Blue Cross and Blue Shield Provider Manual (Kentucky)_recSjJXsDen9Vy51U.pdf' to record recSjJXsDen9Vy51U...\n",
      "✅ Successfully uploaded 'Anthem Blue Cross and Blue Shield Provider Manual (Kentucky)_recSjJXsDen9Vy51U.pdf'.\n",
      "Uploading 'Implants_recfxLHxw0rnagUpU.pdf' to record recfxLHxw0rnagUpU...\n",
      "✅ Successfully uploaded 'Implants_recfxLHxw0rnagUpU.pdf'.\n",
      "Uploading 'CarelonRx Exclusive Specialty List (NY Health Plan)_recS30VdVu3d1CxEI.pdf' to record recS30VdVu3d1CxEI...\n",
      "✅ Successfully uploaded 'CarelonRx Exclusive Specialty List (NY Health Plan)_recS30VdVu3d1CxEI.pdf'.\n",
      "Uploading 'Utilization Management_recRySnZdQbvEo7jq.pdf' to record recRySnZdQbvEo7jq...\n",
      "✅ Successfully uploaded 'Utilization Management_recRySnZdQbvEo7jq.pdf'.\n",
      "Uploading 'Facility Services Information_recGrbxNGCGguRxVE.pdf' to record recGrbxNGCGguRxVE...\n"
     ]
    }
   ],
   "source": [
    "# Airtable mass upload of the data scraped above\n",
    "# adding in pyairtable\n",
    "from pyairtable import Table\n",
    "from pyairtable.formulas import match\n",
    "\n",
    "# building an uploader function\n",
    "def pdf_to_airtable_uploader(folder_path, PAT, base_id, table_id, field_id):\n",
    "    table = Table(PAT, base_id, table_id)\n",
    "    # counting the number of files uploaded to ensure effectiveness\n",
    "    processed_files = 0\n",
    "\n",
    "    # building a looped approach to uploading our documents\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            try:\n",
    "                # first, pulling the Airtable ID appended to each file name (done in the block above)\n",
    "                record_id = filename.rsplit('_', 1)[-1].replace('.pdf', '')\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                print(f\"Uploading '{filename}' to record {record_id}...\")\n",
    "                \n",
    "                # aligning the record ID with the field ID where the document is to be deposited\n",
    "                table.upload_attachment(record_id, field_id, file_path)\n",
    "                \n",
    "                print(f\"✅ Successfully uploaded '{filename}'.\")\n",
    "                processed_files += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {filename}: {e}\")\n",
    "\n",
    "    print(f\"\\nCompleted! Processed {processed_files} files.\")\n",
    "\n",
    "\n",
    "# Airtable access data for this project\n",
    "folder = \"CHIR Policy Reporter Downloads\"\n",
    "airtable_personal_access_token = \"patYtcTKPugBBOk02.167ae1ed1d96262ca9c29b009acbfd660ec00397b0af109876abca0ff87be16e\"\n",
    "airtable_base_id = \"appTq32gWtVc2OctO\"\n",
    "airtable_table_id = \"tbl6Gj7Pj4BCj3viL\"\n",
    "airtable_field_id = \"fldgnqPXS9oeSxfGA\" \n",
    "\n",
    "pdf_to_airtable_uploader(folder, airtable_personal_access_token, airtable_base_id, airtable_table_id, airtable_field_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
